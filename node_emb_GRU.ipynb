{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# dependencies\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, CuDNNGRU, Dense\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## making pairs with negative sampling\n",
    "# generating positive and negative couples using skipgram module from keras\n",
    "seqs = loadmat('seqs.mat')['list3']\n",
    "vocab_size = seqs.max() + 1\n",
    "window_size = 1\n",
    "sampling_table = sequence.make_sampling_table(vocab_size)\n",
    "p1 = []\n",
    "p2 = []\n",
    "out = []    \n",
    "for x in seqs:\n",
    "    couples, labels = sequence.skipgrams(x, vocab_size, window_size=window_size, sampling_table=sampling_table)\n",
    "    for cpl, lbl in zip(couples, labels):\n",
    "        p1.append(cpl[0])\n",
    "        p2.append(cpl[1])\n",
    "        out.append(lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MODEL DESIGN\n",
    "# model variables\n",
    "in_dim = vocab_size\n",
    "red_dim = 600\n",
    "out_dim = 1\n",
    "n_hidden = 300\n",
    "\n",
    "# model components\n",
    "encoder_inputs = Input(shape = (None, in_dim), name = 'encoder_inputs')\n",
    "decoder_inputs = Input(shape = (None, in_dim), name = 'decoder_inputs')\n",
    "pre_dense = Dense(red_dim, activation = 'selu', name = 'pre_dense')\n",
    "encoder = CuDNNGRU(n_hidden, return_sequences = True, return_state = True, name = 'encoder')\n",
    "decoder = CuDNNGRU(n_hidden, return_sequences = True, name = 'decoder')\n",
    "dense = Dense(out_dim, activation = 'sigmoid', name = 'dense')\n",
    "\n",
    "# data flow\n",
    "pre_dense_out1 = pre_dense(encoder_inputs)\n",
    "encoder_out, states = encoder(pre_dense_out1)\n",
    "pre_dense_out2 = pre_dense(decoder_inputs)\n",
    "decoder_out = decoder(pre_dense_out2, initial_state = states)\n",
    "dense_out = dense(decoder_out)\n",
    "\n",
    "# constructing the model\n",
    "training_model = Model([encoder_inputs, decoder_inputs], dense_out)\n",
    "training_model.compile(loss = 'binary_crossentropy', optimizer = 'rmsprop', metrics = ['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FORWARD TRAINING STARTED\n",
      "Epoch 1/25\n",
      "93120/93120 [==============================] - 5s 59us/step - loss: 0.6451 - acc: 0.6089\n",
      "Epoch 2/25\n",
      "93120/93120 [==============================] - 1s 12us/step - loss: 0.4224 - acc: 0.8098\n",
      "Epoch 3/25\n",
      "93120/93120 [==============================] - 1s 12us/step - loss: 0.2807 - acc: 0.8926\n",
      "Epoch 4/25\n",
      "93120/93120 [==============================] - 1s 13us/step - loss: 0.2144 - acc: 0.9235\n",
      "Epoch 5/25\n",
      "93120/93120 [==============================] - 1s 13us/step - loss: 0.1757 - acc: 0.9404\n",
      "Epoch 6/25\n",
      "93120/93120 [==============================] - 1s 13us/step - loss: 0.1489 - acc: 0.9505\n",
      "Epoch 7/25\n",
      "93120/93120 [==============================] - 1s 13us/step - loss: 0.1292 - acc: 0.9586\n",
      "Epoch 8/25\n",
      "93120/93120 [==============================] - 1s 13us/step - loss: 0.1165 - acc: 0.9634\n",
      "Epoch 9/25\n",
      "93120/93120 [==============================] - 1s 12us/step - loss: 0.1029 - acc: 0.9679\n",
      "Epoch 10/25\n",
      "93120/93120 [==============================] - 1s 13us/step - loss: 0.0922 - acc: 0.9719\n",
      "Epoch 11/25\n",
      "93120/93120 [==============================] - 1s 12us/step - loss: 0.0831 - acc: 0.9748\n",
      "Epoch 12/25\n",
      "93120/93120 [==============================] - 1s 12us/step - loss: 0.0743 - acc: 0.9770\n",
      "Epoch 13/25\n",
      "93120/93120 [==============================] - 1s 12us/step - loss: 0.0655 - acc: 0.9809\n",
      "Epoch 14/25\n",
      "93120/93120 [==============================] - 1s 13us/step - loss: 0.0580 - acc: 0.9830\n",
      "Epoch 15/25\n",
      "93120/93120 [==============================] - 1s 13us/step - loss: 0.0516 - acc: 0.9854\n",
      "Epoch 16/25\n",
      "93120/93120 [==============================] - 1s 12us/step - loss: 0.0467 - acc: 0.9869\n",
      "Epoch 17/25\n",
      "93120/93120 [==============================] - 1s 13us/step - loss: 0.0405 - acc: 0.9890\n",
      "Epoch 18/25\n",
      "93120/93120 [==============================] - 1s 13us/step - loss: 0.0379 - acc: 0.9898\n",
      "Epoch 19/25\n",
      "93120/93120 [==============================] - 1s 12us/step - loss: 0.0339 - acc: 0.9913\n",
      "Epoch 20/25\n",
      "93120/93120 [==============================] - 1s 13us/step - loss: 0.0314 - acc: 0.9918\n",
      "Epoch 21/25\n",
      "93120/93120 [==============================] - 1s 13us/step - loss: 0.0293 - acc: 0.9924\n",
      "Epoch 22/25\n",
      "93120/93120 [==============================] - 1s 12us/step - loss: 0.0270 - acc: 0.9932\n",
      "Epoch 23/25\n",
      "93120/93120 [==============================] - 1s 12us/step - loss: 0.0249 - acc: 0.9936\n",
      "Epoch 24/25\n",
      "93120/93120 [==============================] - 1s 12us/step - loss: 0.0250 - acc: 0.9938\n",
      "Epoch 25/25\n",
      "93120/93120 [==============================] - 1s 12us/step - loss: 0.0219 - acc: 0.9948\n",
      "FORWARD TRAINING FINISHED\n",
      "BACKWARD TRAINING STARTED\n",
      "Epoch 1/25\n",
      "93120/93120 [==============================] - 1s 13us/step - loss: 0.0382 - acc: 0.9905\n",
      "Epoch 2/25\n",
      "93120/93120 [==============================] - 1s 12us/step - loss: 0.0284 - acc: 0.9933\n",
      "Epoch 3/25\n",
      "93120/93120 [==============================] - 1s 12us/step - loss: 0.0257 - acc: 0.9937\n",
      "Epoch 4/25\n",
      "93120/93120 [==============================] - 1s 13us/step - loss: 0.0242 - acc: 0.9942\n",
      "Epoch 5/25\n",
      "93120/93120 [==============================] - 1s 13us/step - loss: 0.0230 - acc: 0.9945\n",
      "Epoch 6/25\n",
      "93120/93120 [==============================] - 1s 13us/step - loss: 0.0210 - acc: 0.9952\n",
      "Epoch 7/25\n",
      "93120/93120 [==============================] - 1s 13us/step - loss: 0.0209 - acc: 0.9949\n",
      "Epoch 8/25\n",
      "93120/93120 [==============================] - 1s 13us/step - loss: 0.0196 - acc: 0.9955\n",
      "Epoch 9/25\n",
      "93120/93120 [==============================] - 1s 13us/step - loss: 0.0197 - acc: 0.9954\n",
      "Epoch 10/25\n",
      "93120/93120 [==============================] - 1s 13us/step - loss: 0.0184 - acc: 0.9958\n",
      "Epoch 11/25\n",
      "93120/93120 [==============================] - 1s 13us/step - loss: 0.0183 - acc: 0.9957\n",
      "Epoch 12/25\n",
      "93120/93120 [==============================] - 1s 13us/step - loss: 0.0174 - acc: 0.9962\n",
      "Epoch 13/25\n",
      "93120/93120 [==============================] - 1s 13us/step - loss: 0.0178 - acc: 0.9958\n",
      "Epoch 14/25\n",
      "93120/93120 [==============================] - 1s 13us/step - loss: 0.0166 - acc: 0.9962\n",
      "Epoch 15/25\n",
      "93120/93120 [==============================] - 1s 13us/step - loss: 0.0166 - acc: 0.9963\n",
      "Epoch 16/25\n",
      "93120/93120 [==============================] - 1s 13us/step - loss: 0.0161 - acc: 0.9963\n",
      "Epoch 17/25\n",
      "93120/93120 [==============================] - 1s 13us/step - loss: 0.0154 - acc: 0.9965\n",
      "Epoch 18/25\n",
      "93120/93120 [==============================] - 1s 13us/step - loss: 0.0160 - acc: 0.9966\n",
      "Epoch 19/25\n",
      "93120/93120 [==============================] - 1s 13us/step - loss: 0.0160 - acc: 0.9962\n",
      "Epoch 20/25\n",
      "93120/93120 [==============================] - 1s 13us/step - loss: 0.0155 - acc: 0.9967\n",
      "Epoch 21/25\n",
      "93120/93120 [==============================] - 1s 13us/step - loss: 0.0150 - acc: 0.9968\n",
      "Epoch 22/25\n",
      "93120/93120 [==============================] - 1s 13us/step - loss: 0.0148 - acc: 0.9968\n",
      "Epoch 23/25\n",
      "93120/93120 [==============================] - 1s 13us/step - loss: 0.0144 - acc: 0.9968\n",
      "Epoch 24/25\n",
      "93120/93120 [==============================] - 1s 13us/step - loss: 0.0144 - acc: 0.9968\n",
      "Epoch 25/25\n",
      "93120/93120 [==============================] - 1s 13us/step - loss: 0.0142 - acc: 0.9970\n",
      "BACKWARD TRAINING FINISHED\n"
     ]
    }
   ],
   "source": [
    "## TRAINING\n",
    "# training dependencies\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "# converting into numpy arrays\n",
    "encoder_in = np.asarray(p1, dtype = 'int')\n",
    "decoder_in = np.asarray(p2, dtype = 'int')\n",
    "train_labels = np.asarray(out, dtype = 'int')\n",
    "\n",
    "encoder_in = to_categorical(encoder_in, num_classes = in_dim)\n",
    "decoder_in = to_categorical(decoder_in, num_classes = in_dim)\n",
    "\n",
    "# reshaping into 3D LSTM input\n",
    "encoder_in = np.reshape(encoder_in, (len(encoder_in), 1, in_dim))\n",
    "decoder_in = np.reshape(decoder_in, (len(decoder_in), 1, in_dim))\n",
    "train_labels = np.reshape(train_labels, (len(train_labels), 1, out_dim))\n",
    "\n",
    "# training variables\n",
    "epochs = 25\n",
    "batch_size = 1000\n",
    "\n",
    "# training the data\n",
    "print 'FORWARD TRAINING STARTED'\n",
    "training_model.fit([encoder_in, decoder_in], train_labels, epochs = epochs, batch_size = batch_size, verbose = 1)\n",
    "print 'FORWARD TRAINING FINISHED'\n",
    "\n",
    "print 'BACKWARD TRAINING STARTED'\n",
    "training_model.fit([decoder_in, encoder_in], train_labels, epochs = epochs, batch_size = batch_size, verbose = 1)\n",
    "print 'BACKWARD TRAINING FINISHED'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## retrieving embeddings\n",
    "dense_weights = training_model.layers[2].get_weights()\n",
    "trained_weights = training_model.layers[4].get_weights()\n",
    "for i in range(len(dense_weights)):\n",
    "    trained_weights.insert(i, dense_weights[i])\n",
    "\n",
    "# embedding model components\n",
    "inputs = Input(shape = (None, in_dim), name = 'inputs')\n",
    "pre_dense = Dense(red_dim, activation = 'relu', name = 'pre_dense')\n",
    "lstm_cell = CuDNNGRU(n_hidden, return_sequences = True, name = 'lstm_cell')\n",
    "pre_dense_out = pre_dense(inputs)\n",
    "lstm_out = lstm_cell(pre_dense_out)\n",
    "\n",
    "# building the model.\n",
    "pred_model = Model(inputs, lstm_out)\n",
    "pred_model.set_weights(trained_weights)\n",
    "pred_model.compile(loss = 'binary_crossentropy', optimizer = 'rmsprop', metrics = ['acc'])\n",
    "\n",
    "# creating prediction data\n",
    "pred_data = np.arange(0, in_dim)\n",
    "pred_data = to_categorical(pred_data, num_classes = in_dim)\n",
    "pred_data = np.reshape(pred_data, (len(pred_data), 1, in_dim))\n",
    "\n",
    "# making predictions\n",
    "preds = pred_model.predict(pred_data)\n",
    "req_emb = np.reshape(preds, (len(preds), n_hidden))\n",
    "\n",
    "# saving the embedding as numpy array\n",
    "np.save('node_emb.npy', req_emb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
